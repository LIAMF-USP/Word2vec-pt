{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another implementation of word2vec: this is the first version that I saw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import inspect\n",
    "import string\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing = {}\n",
    "\n",
    "def get_time(f,args):\n",
    "    key =  f.__name__  + \"-\" +\"_\".join([str(arg) for arg in args])\n",
    "    return timing[key]\n",
    "\n",
    "def timeit(method):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        timed.__name__ = method.__name__        \n",
    "        te = time.time()\n",
    "        fkey = method.__name__  + \"-\" +\"_\".join([str(arg) for arg in args])\n",
    "        timing[fkey] = te-ts\n",
    "\n",
    "        #print('%r (%r, %r) %2.2f sec'% (method.__name__, args, kw, te-ts))\n",
    "        return result\n",
    "\n",
    "    return timed\n",
    "\n",
    "class Foo(object):\n",
    "\n",
    "    @timeit\n",
    "    def foo(self, a=2, b=3):\n",
    "        time.sleep(0.2)\n",
    "\n",
    "@timeit\n",
    "def f1(a,b):\n",
    "    return a+b\n",
    "\n",
    "f1(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.430511474609375e-06"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time(f1,[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 15164437\n"
     ]
    }
   ],
   "source": [
    "filename = 'pt96.txt'  \n",
    "file_path = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "file_path = os.path.join(file_path,'data')\n",
    "file_path = os.path.join(file_path,filename)\n",
    "\n",
    "@timeit\n",
    "def read_data(file_path):\n",
    "    dic_trans = {key: None for key in string.punctuation}\n",
    "    translator = str.maketrans(dic_trans)\n",
    "    words = []\n",
    "    with open(file_path) as inputfile:\n",
    "        for line in inputfile:\n",
    "            aux = line.translate(translator)\n",
    "            words.extend(aux.strip().split())\n",
    "    return words\n",
    "\n",
    "words = read_data(file_path)\n",
    "print('Data size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1-1_2': 1.430511474609375e-06,\n",
       " 'read_data-/home/felipe/Desktop/word2vec-basic/source/data/pt96.txt': 8.594845056533813}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.594845056533813"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_time(read_data,[file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 1077275], ('de', 763544), ('e', 422852), ('a', 422319), ('o', 335045)]\n",
      "Sample data [7825, 18650, 0, 5514, 18650, 0, 9977, 6489, 8555, 2190]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18650 -> 0\n",
      "Contos -> UNK\n",
      "18650 -> 7825\n",
      "Contos -> Conto\n",
      "0 -> 5514\n",
      "UNK -> 1870\n",
      "0 -> 18650\n",
      "UNK -> Contos\n",
      "5514 -> 18650\n",
      "1870 -> Contos\n",
      "5514 -> 0\n",
      "1870 -> UNK\n",
      "18650 -> 0\n",
      "Contos -> UNK\n",
      "18650 -> 5514\n",
      "Contos -> 1870\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(int(batch_size / num_skips)):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], '->', labels[i, 0])\n",
    "  print(reverse_dictionary[batch[i]], '->', reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases,train_labels, embed,num_sampled, vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 7.24202442169\n",
      "Nearest to é: Mor, Mortensen, Juvenis, Católicas, Bot, Diretrizes, abrupta, preservada,\n",
      "Nearest to com: 160, recorre, Stanislaw, Prudente, Katie, vermelhos, Saíram, Dolorosa,\n",
      "Nearest to mas: séria, 249, raptor, explorava, camisas, síria, kgkg, pilhas,\n",
      "Nearest to o: Mitsubishi, calorífico, laborais, Prússia, 54°, Paget, colinas, Iniciando,\n",
      "Nearest to da: evangelizar, cedo, Alt, Elenco, ser, Inferno, abandonadas, Caitlyn,\n",
      "Nearest to a: Third, Ennio, energéticas, Josefa, Musashi, tático, dependerá, 2005,\n",
      "Nearest to nas: cabida, trimestre, apagamento, outros1, insurgentes, publicadores, Nicol, Punho,\n",
      "Nearest to pode: Ariano, realizando, sujeitos, faísca, Boyz, Zeratul, inverno, insectos,\n",
      "Nearest to entre: Type, Corvo, Monetária, inventaram, apontamentos, Powers, casarão, spins,\n",
      "Nearest to ter: Abecásia, paciência, paciente, Germânica, extremidades, escrevente, aliado, cause,\n",
      "Nearest to ao: Introduction, ingresso, participava, resolutamente, industria, intervenientes, triplicou, Rights,\n",
      "Nearest to sua: altiva, relações, ostenta, Hornet, Iwata, brancas, estenderia, incontrolável,\n",
      "Nearest to Os: obtida, Dean, ligeiras, reguladora, 675, THE, reconquistou, Angelica,\n",
      "Nearest to seus: and, Antíoco, psicodélico, Barret, associativo, 1778, internacional, formarem,\n",
      "Nearest to UNK: fecha, Sérvios, Figo, garrafa, abrigo, costumava, Berwyn, Escócia,\n",
      "Nearest to Na: englobava, 135, canônico, Canadá•, Kurosaki, 40º, Florestal, Pyongyang,\n",
      "Average loss at step 2000 : 4.10115451372\n",
      "Average loss at step 4000 : 3.59156979668\n",
      "Average loss at step 6000 : 3.39001337063\n",
      "Average loss at step 8000 : 3.23476593554\n",
      "Average loss at step 10000 : 3.15411274743\n",
      "Nearest to é: era, É, seria, Río, fosse, Coram, foi, Diretora,\n",
      "Nearest to com: para, Escandinávia, sobre, artísticas, e, mentalmente, trator, constantemente,\n",
      "Nearest to mas: Mas, e, porque, que, Eu, Se, enfrentou, 1234,\n",
      "Nearest to o: do, ao, um, este, no, esse, aquele, O,\n",
      "Nearest to da: A, do, na, ogã, Stott, à, das, retrata,\n",
      "Nearest to a: esta, Union·, uma, à, A, na, aquela, McCoy,\n",
      "Nearest to nas: das, as, outros1, Nicol, Punho, cabida, combinados, insistisse,\n",
      "Nearest to pode: sujeitos, podia, ChampanhaArdenas, Zeratul, vá, Huega, faísca, serei,\n",
      "Nearest to entre: casarão, Type, Monetária, F102, apontamentos, houve, Colônias, norueguês,\n",
      "Nearest to ter: 18281834, Germânica, Formicinae1, Bellof, DO, if, paciência, desfezse,\n",
      "Nearest to ao: no, o, pelo, à, O, do, extinto, Ao,\n",
      "Nearest to sua: minha, tua, nossa, garotas, guiada, verdadeira, vendidos, consular,\n",
      "Nearest to Os: os, dos, aos, servas, atos, nos, achandose, convertendo,\n",
      "Nearest to seus: and, circular, dois, meus, fizessem, formarem, psicodélico, alinhar,\n",
      "Nearest to UNK: Iruka, tendiam, carenagem, deram, Fulano, Tarde, retirarse, contribui,\n",
      "Nearest to Na: coesão, Göttingen, Mackay, fores, na, à, subjugar, da,\n",
      "Average loss at step 12000 : 3.19418238175\n",
      "Average loss at step 14000 : 3.29213963938\n",
      "Average loss at step 16000 : 3.41133183557\n",
      "Average loss at step 18000 : 3.35759058511\n",
      "Average loss at step 20000 : 3.3364330883\n",
      "Nearest to é: era, foi, É, seria, seja, fosse, tinha, Era,\n",
      "Nearest to com: sobre, para, contra, como, Escandinávia, entre, e, discutindo,\n",
      "Nearest to mas: Mas, e, porque, E, É, nem, Eu, que,\n",
      "Nearest to o: O, um, este, ao, no, esse, pelo, do,\n",
      "Nearest to da: do, A, à, das, Da, de, desta, pela,\n",
      "Nearest to a: à, uma, pela, na, essa, A, esta, aquela,\n",
      "Nearest to nas: as, das, às, outros1, Punho, essas, cabida, cantinho,\n",
      "Nearest to pode: podia, sem, devia, 1877, deve, podem, vá, pude,\n",
      "Nearest to entre: com, casarão, apontamentos, manifestando, F102, para, arbitrariamente, Powers,\n",
      "Nearest to ter: ser, dar, desfezse, haver, Vanderbilt, fazer, Germânica, reunidos,\n",
      "Nearest to ao: à, o, no, pelo, do, O, Ao, às,\n",
      "Nearest to sua: minha, nossa, tua, uma, outra, primeira, mesma, própria,\n",
      "Nearest to Os: dos, os, aos, O, A, atos, convertendo, achandose,\n",
      "Nearest to seus: nossos, meus, and, outros, dois, Tabocas, os, alinhar,\n",
      "Nearest to UNK: Wiesel·, Presbitério, Juro, Brazelton, Belarmino, guarnição, Neji, controlado,\n",
      "Nearest to Na: A, à, na, Göttingen, coesão, fores, Mackay, Blades,\n",
      "Average loss at step 22000 : 3.29911150301\n",
      "Average loss at step 24000 : 3.18209514856\n",
      "Average loss at step 26000 : 3.20849800098\n",
      "Average loss at step 28000 : 3.01258108842\n",
      "Average loss at step 30000 : 3.01754852772\n",
      "Nearest to é: era, É, seja, seria, Era, fosse, foi, será,\n",
      "Nearest to com: sobre, entre, Com, Escandinávia, contra, artísticas, mentalmente, para,\n",
      "Nearest to mas: Mas, porque, e, E, É, Eu, Se, que,\n",
      "Nearest to o: O, do, um, aquele, este, pelo, esse, ao,\n",
      "Nearest to da: do, na, à, daquela, de, dos, Da, desta,\n",
      "Nearest to a: à, na, pela, A, da, esta, aquela, VLP,\n",
      "Nearest to nas: as, às, pelas, essas, cabida, das, Punho, As,\n",
      "Nearest to pode: podia, deve, podem, devia, queria, pude, quer, posso,\n",
      "Nearest to entre: com, sobre, contra, casarão, manifestando, Langenbach, CP, apontamentos,\n",
      "Nearest to ter: haver, ser, dar, Vanderbilt, desfezse, fazer, tinha, Germânica,\n",
      "Nearest to ao: à, Ao, no, aos, pelo, às, Die, o,\n",
      "Nearest to sua: minha, tua, nossa, outra, primeira, própria, uma, guiada,\n",
      "Nearest to Os: dos, os, aos, atos, nos, são, achandose, De,\n",
      "Nearest to seus: meus, nossos, outros, os, dois, and, todos, alguns,\n",
      "Nearest to UNK: dá, racismo, fala, Parecialhe, chamou, Theato, ordenado, ninjas,\n",
      "Nearest to Na: na, A, Göttingen, da, Mackay, informática, fores, Blades,\n",
      "Average loss at step 32000 : 3.28599376005\n",
      "Average loss at step 34000 : 3.02901287365\n",
      "Average loss at step 36000 : 2.90135173184\n",
      "Average loss at step 38000 : 2.8397180078\n",
      "Average loss at step 40000 : 3.32563014388\n",
      "Nearest to é: era, foi, É, seria, Era, seja, será, ser,\n",
      "Nearest to com: entre, sobre, para, contra, e, Com, de, durante,\n",
      "Nearest to mas: Mas, e, E, porque, Eu, que, Os, encarnada,\n",
      "Nearest to o: os, um, a, O, do, ele, no, esse,\n",
      "Nearest to da: na, do, A, desta, de, das, daquela, Da,\n",
      "Nearest to a: o, à, os, pela, uma, as, na, A,\n",
      "Nearest to nas: as, pelas, às, das, essas, As, no, em,\n",
      "Nearest to pode: podia, podem, deve, devia, pude, poderá, queria, posso,\n",
      "Nearest to entre: com, sobre, CP, contra, CAC, para, manifestando, como,\n",
      "Nearest to ter: tem, haver, tinha, ser, dar, desfezse, tinham, Vanderbilt,\n",
      "Nearest to ao: à, às, no, aos, pelo, do, O, Ao,\n",
      "Nearest to sua: minha, tua, nossa, uma, essa, outra, assírias, terranos,\n",
      "Nearest to Os: os, nos, dos, são, aos, O, A, No,\n",
      "Nearest to seus: nossos, meus, outros, primeiros, novos, dois, profética, uns,\n",
      "Nearest to UNK: e, homo, Psycho, válidas, 1425, 2, CL, detêlo,\n",
      "Nearest to Na: na, A, informática, da, efectuadas, Göttingen, sobreveio, JPN,\n",
      "Average loss at step 42000 : 3.61546940424\n",
      "Average loss at step 44000 : 3.32075817269\n",
      "Average loss at step 46000 : 3.22069478229\n",
      "Average loss at step 48000 : 3.28078130001\n",
      "Average loss at step 50000 : 3.15506143954\n",
      "Nearest to é: É, era, Era, seria, foi, será, seja, tem,\n",
      "Nearest to com: sobre, Com, entre, contra, para, durante, quando, perseguia,\n",
      "Nearest to mas: Mas, e, porque, Ele, que, E, porém, pois,\n",
      "Nearest to o: um, os, esse, este, a, O, as, aquele,\n",
      "Nearest to da: do, A, Da, na, dos, Na, daquela, cujo,\n",
      "Nearest to a: à, o, essa, pela, A, uma, sua, esta,\n",
      "Nearest to nas: no, pelas, as, às, na, As, Sacerdote, essas,\n",
      "Nearest to pode: podia, podem, deve, pude, queria, poderá, devia, vai,\n",
      "Nearest to entre: contra, com, sobre, desde, Durante, Entre, CP, incluindo,\n",
      "Nearest to ter: ser, haver, tinha, tem, tinham, tivesse, tendo, dar,\n",
      "Nearest to ao: à, aos, às, Ao, pelo, Die, no, o,\n",
      "Nearest to sua: minha, tua, nossa, essa, uma, outra, toda, Sua,\n",
      "Nearest to Os: os, nos, dos, aos, De, Nos, pelos, A,\n",
      "Nearest to seus: nossos, meus, outros, dois, os, alguns, novos, diversos,\n",
      "Nearest to UNK: Hikaru, homo, ferroviária, queridos, Bráulio, Árabe, Lindsay, Ecthelion,\n",
      "Nearest to Na: na, da, A, Os, informática, z, sobreveio, efectuadas,\n",
      "Average loss at step 52000 : 3.14761687896\n",
      "Average loss at step 54000 : 2.93546519592\n",
      "Average loss at step 56000 : 3.20719841878\n",
      "Average loss at step 58000 : 3.35836375082\n",
      "Average loss at step 60000 : 3.20701274639\n",
      "Nearest to é: era, foi, É, seria, Era, possui, tem, Foi,\n",
      "Nearest to com: entre, contra, Com, durante, sobre, quando, Segundo, enquanto,\n",
      "Nearest to mas: porque, e, Mas, porém, Ele, que, pois, Ela,\n",
      "Nearest to o: um, os, este, seu, as, a, esse, aquele,\n",
      "Nearest to da: do, dos, na, Da, Na, das, A, daquela,\n",
      "Nearest to a: à, o, uma, os, A, redações, tentou, essa,\n",
      "Nearest to nas: pelas, às, as, no, das, nos, na, WPP,\n",
      "Nearest to pode: podia, podem, deve, queria, poderá, pude, deveria, devia,\n",
      "Nearest to entre: com, contra, Entre, desde, sobre, incluindo, Durante, durante,\n",
      "Nearest to ter: ser, tinha, haver, tinham, tem, havia, tivesse, tendo,\n",
      "Nearest to ao: às, à, aos, Ao, no, pelo, do, portáteis,\n",
      "Nearest to sua: minha, tua, nossa, essa, uma, toda, Sua, primeira,\n",
      "Nearest to Os: os, dos, nos, Nos, Darleane, 52km², pelos, Estes,\n",
      "Nearest to seus: nossos, os, outros, alguns, meus, muitos, esses, todos,\n",
      "Nearest to UNK: milhas, Jim, migrando, esculpida, ocorrida, absorvem, viraram, esmagado,\n",
      "Nearest to Na: na, da, A, informática, As, nessa, Os, cercar,\n",
      "Average loss at step 62000 : 3.19627266887\n",
      "Average loss at step 64000 : 3.29826178104\n",
      "Average loss at step 66000 : 3.18899010938\n",
      "Average loss at step 68000 : 3.1528013102\n",
      "Average loss at step 70000 : 3.30182132274\n",
      "Nearest to é: foi, É, era, Era, seria, possui, será, tem,\n",
      "Nearest to com: Com, entre, sobre, contra, durante, sob, mentalmente, após,\n",
      "Nearest to mas: porque, Mas, porém, e, pois, Ele, Pois, É,\n",
      "Nearest to o: um, aquele, esse, este, as, Zanna, a, seu,\n",
      "Nearest to da: do, A, de, na, dos, Da, das, Na,\n",
      "Nearest to a: à, A, na, o, àqueles, Itá, instruído, aquela,\n",
      "Nearest to nas: as, pelas, às, das, nos, na, no, WPP,\n",
      "Nearest to pode: podia, podem, deve, queria, poderá, quer, devia, deveria,\n",
      "Nearest to entre: com, contra, Entre, desde, sobre, durante, incluindo, Durante,\n",
      "Nearest to ter: ser, haver, tinha, havia, tinham, tenha, tivesse, tem,\n",
      "Nearest to ao: às, à, aos, Ao, no, do, Celso, Circuncisão,\n",
      "Nearest to sua: minha, nossa, tua, uma, essa, primeira, Sua, outra,\n",
      "Nearest to Os: dos, os, pelos, nos, Nos, O, Estes, Na,\n",
      "Nearest to seus: nossos, os, outros, meus, alguns, primeiros, esses, dois,\n",
      "Nearest to UNK: Ribeiro, cariz, terapia, look, esculpida, Rai, Za, ELN,\n",
      "Nearest to Na: na, A, Nesta, numa, Os, As, da, Compromisso,\n",
      "Average loss at step 72000 : 3.26469008116\n",
      "Average loss at step 74000 : 3.20081851935\n",
      "Average loss at step 76000 : 2.99648466452\n",
      "Average loss at step 78000 : 3.07070053297\n",
      "Average loss at step 80000 : 2.98773998216\n",
      "Nearest to é: É, era, foi, será, Era, tem, seria, possui,\n",
      "Nearest to com: Com, entre, sob, durante, sobre, contra, enquanto, Durante,\n",
      "Nearest to mas: porque, e, porém, Mas, pois, Ele, Pois, Ela,\n",
      "Nearest to o: esse, este, um, aquele, as, pelo, Zanna, seu,\n",
      "Nearest to da: Da, do, desta, dessa, dos, daquela, condicionado, supervisionada,\n",
      "Nearest to a: à, esta, essa, aquela, os, o, uma, mangaká,\n",
      "Nearest to nas: às, pelas, no, nos, as, na, As, das,\n",
      "Nearest to pode: podia, podem, deve, queria, poderá, devem, deveria, poderia,\n",
      "Nearest to entre: com, Entre, contra, durante, desde, Dentre, Durante, sobre,\n",
      "Nearest to ter: haver, ser, tinha, tenha, havia, tivesse, tinham, têm,\n",
      "Nearest to ao: à, às, aos, Ao, no, pelo, Vital, Matheus,\n",
      "Nearest to sua: minha, tua, essa, nossa, Sua, uma, primeira, toda,\n",
      "Nearest to Os: os, dos, pelos, nos, Estes, Nos, O, Ochrophyta,\n",
      "Nearest to seus: nossos, meus, outros, alguns, os, vários, dois, diversos,\n",
      "Nearest to UNK: esculpida, conjectura, homo, Rosewood, December, Considera, Chadwick, convida,\n",
      "Nearest to Na: na, A, numa, Nesta, No, As, nessa, Os,\n",
      "Average loss at step 82000 : 2.98583302614\n",
      "Average loss at step 84000 : 3.03405703211\n",
      "Average loss at step 86000 : 3.04224917179\n",
      "Average loss at step 88000 : 3.14669786745\n",
      "Average loss at step 90000 : 3.02471140087\n",
      "Nearest to é: É, foi, era, seria, será, possui, tem, fosse,\n",
      "Nearest to com: Com, entre, durante, usando, sobre, sob, após, para,\n",
      "Nearest to mas: porque, Mas, porém, pois, e, Ele, Quando, onde,\n",
      "Nearest to o: esse, um, este, a, os, aquele, O, as,\n",
      "Nearest to da: do, A, desta, na, dos, daquela, dessa, deste,\n",
      "Nearest to a: à, essa, o, esta, uma, aquela, os, as,\n",
      "Nearest to nas: as, às, pelas, As, na, no, das, nos,\n",
      "Nearest to pode: podia, podem, deve, poderá, queria, poderia, deveria, devem,\n",
      "Nearest to entre: Entre, contra, com, Dentre, desde, durante, incluindo, Durante,\n",
      "Nearest to ter: tinha, haver, ser, havia, tenha, tem, tivesse, tinham,\n",
      "Nearest to ao: à, às, aos, Ao, no, Vital, pelo, Corner,\n",
      "Nearest to sua: minha, tua, nossa, uma, Sua, essa, cuja, primeira,\n",
      "Nearest to Os: os, pelos, dos, O, Estes, Outros, Seus, A,\n",
      "Nearest to seus: outros, nossos, os, alguns, meus, diversos, novos, dois,\n",
      "Nearest to UNK: But, Rue, LU, Medo, gravando, Wadoryu, Tenho, Marjorie,\n",
      "Nearest to Na: na, A, Nesta, No, numa, Os, Ainda, nessa,\n",
      "Average loss at step 92000 : 3.0401601088\n",
      "Average loss at step 94000 : 3.04939770493\n",
      "Average loss at step 96000 : 2.96525157035\n",
      "Average loss at step 98000 : 2.94472486678\n",
      "Average loss at step 100000 : 2.88850576195\n",
      "Nearest to é: É, seria, foi, será, era, fosse, tem, possui,\n",
      "Nearest to com: entre, Com, durante, sob, usando, sobre, enquanto, Durante,\n",
      "Nearest to mas: Mas, porque, porém, pois, e, Ele, Quando, Porém,\n",
      "Nearest to o: esse, um, aquele, este, seu, os, a, Zanna,\n",
      "Nearest to da: do, desta, Da, A, dessa, daquela, condicionado, abrupto,\n",
      "Nearest to a: à, essa, o, uma, aquela, Itá, esta, Kastler,\n",
      "Nearest to nas: na, às, As, pelas, as, nos, das, no,\n",
      "Nearest to pode: podia, podem, deve, queria, poderá, poderia, deveria, quer,\n",
      "Nearest to entre: Entre, com, contra, Dentre, incluindo, desde, sobre, sob,\n",
      "Nearest to ter: haver, ser, tinha, havia, tenha, terem, tinham, teria,\n",
      "Nearest to ao: à, às, Ao, aos, no, Vital, pelo, 721,\n",
      "Nearest to sua: minha, nossa, tua, uma, Sua, toda, cuja, essa,\n",
      "Nearest to Os: os, pelos, Outros, Johannes, dos, Na, A, Estes,\n",
      "Nearest to seus: outros, os, nossos, meus, alguns, novos, diversos, vários,\n",
      "Nearest to UNK: Hank, Spencer, reproduziu, Jordan, Chadwick, volúpia, temperados, Pound,\n",
      "Nearest to Na: na, A, No, Nesta, As, numa, Os, nessa,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step\", step, \":\", average_loss)\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = \"Nearest to %s:\" % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = \"%s %s,\" % (log, close_word)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def analogy(w1,w2,w3,index_to_word,word_to_index, embeddings):\n",
    "    a = embeddings[word_to_index[w1]]\n",
    "    b = embeddings[word_to_index[w2]]\n",
    "    c = embeddings[word_to_index[w3]]\n",
    "    def apply_dot(x):\n",
    "        return b.dot(x) - a.dot(x) + c.dot(x)\n",
    "    all_results = [(apply_dot(w), index) for index, w in enumerate(embeddings) if (index!= word_to_index[w1] and index!= word_to_index[w2] and index!= word_to_index[w3])]\n",
    "    all_results.sort(reverse=True)\n",
    "    result = [(index_to_word[index],value) for (value,index) in all_results[0:11]]\n",
    "    return result[0][0]  \n",
    "\n",
    "def top_k_sim(vector, index_to_word, embeddings,k=10):\n",
    "    all_sim = [(1 - spatial.distance.cosine(vector, w),index) for index, w in enumerate(embeddings)]\n",
    "    all_sim.sort(reverse =True)\n",
    "    result = [(index_to_word[index],value) for (value,index) in all_sim[0:k+1]]\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mulher', 0.99999994039535522),\n",
       " ('moça', 0.4613415002822876),\n",
       " ('filha', 0.45169356465339661),\n",
       " ('Bednorz', 0.39365583658218384),\n",
       " ('mãe', 0.38549751043319702),\n",
       " ('tia', 0.37841418385505676),\n",
       " ('carta', 0.37613511085510254),\n",
       " ('visita', 0.37451835718860371),\n",
       " ('Heloísa', 0.36902861368315054),\n",
       " ('derrubado', 0.35476463167456584),\n",
       " ('Ignaz', 0.35475873081438136)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mulher = final_embeddings[dictionary['mulher']]\n",
    "top_k_sim(mulher,reverse_dictionary,final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crockett'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"homem\",\"mulher\",\"rei\",reverse_dictionary,dictionary,final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_file=\"analogies_pt.txt\"\n",
    "vocab = set(dictionary.keys())\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "prefix = os.path.join(currentdir, \"evaluation\")\n",
    "file_path = os.path.join(prefix, eval_file)\n",
    "valid_tests = 0\n",
    "correct_answer = 0\n",
    "total_lines = 0\n",
    "total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analogy --> ['argentina', 'peso', 'europa', 'euro']\n",
      "prediction --> Standard\n",
      "current_line:25470, duration = 0.17199397087097168\n",
      "Analogy --> ['argentina', 'peso', 'índia', 'rupia']\n",
      "prediction --> presenças\n",
      "current_line:25472, duration = 0.17736601829528809\n",
      "Analogy --> ['europa', 'euro', 'índia', 'rupia']\n",
      "prediction --> desporto\n",
      "current_line:25694, duration = 0.17482328414916992\n",
      "Analogy --> ['europa', 'euro', 'argentina', 'peso']\n",
      "prediction --> psicóloga\n",
      "current_line:25714, duration = 0.1721484661102295\n",
      "Analogy --> ['índia', 'rupia', 'argentina', 'peso']\n",
      "prediction --> querubins\n",
      "current_line:25770, duration = 0.17726445198059082\n",
      "Analogy --> ['índia', 'rupia', 'europa', 'euro']\n",
      "prediction --> 153\n",
      "current_line:25778, duration = 0.17674016952514648\n",
      "Analogy --> ['rapaz', 'moça', 'irmãos', 'irmãs']\n",
      "prediction --> abrigava\n",
      "current_line:28737, duration = 0.17703747749328613\n",
      "Analogy --> ['rapaz', 'moça', 'pai', 'mãe']\n",
      "prediction --> mãe\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28738, duration = 0.1715543270111084\n",
      "Analogy --> ['rapaz', 'moça', 'pai', 'mãe']\n",
      "prediction --> mãe\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28739, duration = 0.17288589477539062\n",
      "Analogy --> ['rapaz', 'moça', 'avô', 'avó']\n",
      "prediction --> carnes\n",
      "current_line:28740, duration = 0.17320799827575684\n",
      "Analogy --> ['rapaz', 'moça', 'avô', 'avó']\n",
      "prediction --> carnes\n",
      "current_line:28741, duration = 0.17405939102172852\n",
      "Analogy --> ['rapaz', 'moça', 'neto', 'neta']\n",
      "prediction --> MolotovRibbentrop\n",
      "current_line:28742, duration = 0.17460203170776367\n",
      "Analogy --> ['rapaz', 'moça', 'noivo', 'noiva']\n",
      "prediction --> reformadores\n",
      "current_line:28743, duration = 0.17282891273498535\n",
      "Analogy --> ['rapaz', 'moça', 'ele', 'ela']\n",
      "prediction --> ela\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28744, duration = 0.21559810638427734\n",
      "Analogy --> ['rapaz', 'moça', 'dele', 'dela']\n",
      "prediction --> nutrição\n",
      "current_line:28745, duration = 0.1839590072631836\n",
      "Analogy --> ['rapaz', 'moça', 'marido', 'mulher']\n",
      "prediction --> apreciação\n",
      "current_line:28746, duration = 0.25206899642944336\n",
      "Analogy --> ['rapaz', 'moça', 'rei', 'rainha']\n",
      "prediction --> madrinha\n",
      "current_line:28747, duration = 0.20218944549560547\n",
      "Analogy --> ['rapaz', 'moça', 'homem', 'mulher']\n",
      "prediction --> nutrição\n",
      "current_line:28748, duration = 0.1922609806060791\n",
      "Analogy --> ['rapaz', 'moça', 'sobrinho', 'sobrinha']\n",
      "prediction --> Barata\n",
      "current_line:28749, duration = 0.17081022262573242\n",
      "Analogy --> ['rapaz', 'moça', 'príncipe', 'princesa']\n",
      "prediction --> Vestenbergsgreuth\n",
      "current_line:28750, duration = 0.1744389533996582\n",
      "Analogy --> ['rapaz', 'moça', 'filho', 'filha']\n",
      "prediction --> Municípios\n",
      "current_line:28751, duration = 0.1703653335571289\n",
      "Analogy --> ['rapaz', 'moça', 'padrasto', 'madrasta']\n",
      "prediction --> enfadonha\n",
      "current_line:28754, duration = 0.17387676239013672\n",
      "Analogy --> ['rapaz', 'moça', 'tio', 'tia']\n",
      "prediction --> apreciação\n",
      "current_line:28756, duration = 0.17092132568359375\n",
      "Analogy --> ['irmão', 'irmã', 'irmãos', 'irmãs']\n",
      "prediction --> farão\n",
      "current_line:28757, duration = 0.17537856101989746\n",
      "Analogy --> ['irmão', 'irmã', 'pai', 'mãe']\n",
      "prediction --> mãe\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28758, duration = 0.17140555381774902\n",
      "Analogy --> ['irmão', 'irmã', 'pai', 'mãe']\n",
      "prediction --> mãe\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28759, duration = 0.17466354370117188\n",
      "Analogy --> ['irmão', 'irmã', 'avô', 'avó']\n",
      "prediction --> barra\n",
      "current_line:28760, duration = 0.17260026931762695\n",
      "Analogy --> ['irmão', 'irmã', 'avô', 'avó']\n",
      "prediction --> barra\n",
      "current_line:28761, duration = 0.1725912094116211\n",
      "Analogy --> ['irmão', 'irmã', 'neto', 'neta']\n",
      "prediction --> rebeldia\n",
      "current_line:28762, duration = 0.1714019775390625\n",
      "Analogy --> ['irmão', 'irmã', 'noivo', 'noiva']\n",
      "prediction --> Itá\n",
      "current_line:28763, duration = 0.17062711715698242\n",
      "Analogy --> ['irmão', 'irmã', 'ele', 'ela']\n",
      "prediction --> ela\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28764, duration = 0.17549347877502441\n",
      "Analogy --> ['irmão', 'irmã', 'dele', 'dela']\n",
      "prediction --> dela\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28765, duration = 0.171156644821167\n",
      "Analogy --> ['irmão', 'irmã', 'marido', 'mulher']\n",
      "prediction --> Conquest\n",
      "current_line:28766, duration = 0.17177796363830566\n",
      "Analogy --> ['irmão', 'irmã', 'rei', 'rainha']\n",
      "prediction --> seladas\n",
      "current_line:28767, duration = 0.1707603931427002\n",
      "Analogy --> ['irmão', 'irmã', 'homem', 'mulher']\n",
      "prediction --> trabalham\n",
      "current_line:28768, duration = 0.17005228996276855\n",
      "Analogy --> ['irmão', 'irmã', 'sobrinho', 'sobrinha']\n",
      "prediction --> ascensão\n",
      "current_line:28769, duration = 0.1696174144744873\n",
      "Analogy --> ['irmão', 'irmã', 'príncipe', 'princesa']\n",
      "prediction --> Conquest\n",
      "current_line:28770, duration = 0.17238211631774902\n",
      "Analogy --> ['irmão', 'irmã', 'filho', 'filha']\n",
      "prediction --> Dantzig·\n",
      "current_line:28771, duration = 0.16937518119812012\n",
      "Analogy --> ['irmão', 'irmã', 'padrasto', 'madrasta']\n",
      "prediction --> Monk\n",
      "current_line:28774, duration = 0.16961884498596191\n",
      "Analogy --> ['irmão', 'irmã', 'tio', 'tia']\n",
      "prediction --> machado\n",
      "current_line:28776, duration = 0.17067980766296387\n",
      "Analogy --> ['irmão', 'irmã', 'rapaz', 'moça']\n",
      "prediction --> exceção\n",
      "current_line:28777, duration = 0.17016196250915527\n",
      "Analogy --> ['irmãos', 'irmãs', 'pai', 'mãe']\n",
      "prediction --> trabalho\n",
      "current_line:28778, duration = 0.16962504386901855\n",
      "Analogy --> ['irmãos', 'irmãs', 'pai', 'mãe']\n",
      "prediction --> trabalho\n",
      "current_line:28779, duration = 0.1740727424621582\n",
      "Analogy --> ['irmãos', 'irmãs', 'avô', 'avó']\n",
      "prediction --> aspecto\n",
      "current_line:28780, duration = 0.1926286220550537\n",
      "Analogy --> ['irmãos', 'irmãs', 'avô', 'avó']\n",
      "prediction --> aspecto\n",
      "current_line:28781, duration = 0.1857004165649414\n",
      "Analogy --> ['irmãos', 'irmãs', 'neto', 'neta']\n",
      "prediction --> viajar\n",
      "current_line:28782, duration = 0.19711732864379883\n",
      "Analogy --> ['irmãos', 'irmãs', 'noivo', 'noiva']\n",
      "prediction --> mote\n",
      "current_line:28783, duration = 0.17007231712341309\n",
      "Analogy --> ['irmãos', 'irmãs', 'ele', 'ela']\n",
      "prediction --> ela\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28784, duration = 0.20270204544067383\n",
      "Analogy --> ['irmãos', 'irmãs', 'dele', 'dela']\n",
      "prediction --> munido\n",
      "current_line:28785, duration = 0.1792445182800293\n",
      "Analogy --> ['irmãos', 'irmãs', 'marido', 'mulher']\n",
      "prediction --> trabalho\n",
      "current_line:28786, duration = 0.18602895736694336\n",
      "Analogy --> ['irmãos', 'irmãs', 'rei', 'rainha']\n",
      "prediction --> asiáticos\n",
      "current_line:28787, duration = 0.19381141662597656\n",
      "Analogy --> ['irmãos', 'irmãs', 'homem', 'mulher']\n",
      "prediction --> tratamento\n",
      "current_line:28788, duration = 0.17142653465270996\n",
      "Analogy --> ['irmãos', 'irmãs', 'sobrinho', 'sobrinha']\n",
      "prediction --> trabalho\n",
      "current_line:28789, duration = 0.16948533058166504\n",
      "Analogy --> ['irmãos', 'irmãs', 'príncipe', 'princesa']\n",
      "prediction --> recorde\n",
      "current_line:28790, duration = 0.17135357856750488\n",
      "Analogy --> ['irmãos', 'irmãs', 'filho', 'filha']\n",
      "prediction --> pai\n",
      "current_line:28791, duration = 0.16886329650878906\n",
      "Analogy --> ['irmãos', 'irmãs', 'padrasto', 'madrasta']\n",
      "prediction --> luas\n",
      "current_line:28794, duration = 0.1695098876953125\n",
      "Analogy --> ['irmãos', 'irmãs', 'tio', 'tia']\n",
      "prediction --> mote\n",
      "current_line:28796, duration = 0.16948437690734863\n",
      "Analogy --> ['irmãos', 'irmãs', 'rapaz', 'moça']\n",
      "prediction --> uvas\n",
      "current_line:28797, duration = 0.17065167427062988\n",
      "Analogy --> ['irmãos', 'irmãs', 'irmão', 'irmã']\n",
      "prediction --> mote\n",
      "current_line:28798, duration = 0.17096161842346191\n",
      "Analogy --> ['pai', 'mãe', 'pai', 'mãe']\n",
      "prediction --> morte\n",
      "current_line:28799, duration = 0.17012619972229004\n",
      "Analogy --> ['pai', 'mãe', 'avô', 'avó']\n",
      "prediction --> avó\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28800, duration = 0.17099571228027344\n",
      "Analogy --> ['pai', 'mãe', 'avô', 'avó']\n",
      "prediction --> avó\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28801, duration = 0.1694808006286621\n",
      "Analogy --> ['pai', 'mãe', 'neto', 'neta']\n",
      "prediction --> Sallie\n",
      "current_line:28802, duration = 0.1726977825164795\n",
      "Analogy --> ['pai', 'mãe', 'noivo', 'noiva']\n",
      "prediction --> avó\n",
      "current_line:28803, duration = 0.16946077346801758\n",
      "Analogy --> ['pai', 'mãe', 'ele', 'ela']\n",
      "prediction --> ela\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28804, duration = 0.17264032363891602\n",
      "Analogy --> ['pai', 'mãe', 'dele', 'dela']\n",
      "prediction --> dela\n",
      "\n",
      "YESSSSSSSSSSSSSSSSSSSSS\n",
      "\n",
      "current_line:28805, duration = 0.17030596733093262\n",
      "Analogy --> ['pai', 'mãe', 'marido', 'mulher']\n",
      "prediction --> teimosia\n",
      "current_line:28806, duration = 0.17899584770202637\n",
      "Analogy --> ['pai', 'mãe', 'rei', 'rainha']\n",
      "prediction --> Varig\n",
      "current_line:28807, duration = 0.1710660457611084\n",
      "Analogy --> ['pai', 'mãe', 'homem', 'mulher']\n",
      "prediction --> auditório\n",
      "current_line:28808, duration = 0.19236087799072266\n",
      "Analogy --> ['pai', 'mãe', 'sobrinho', 'sobrinha']\n",
      "prediction --> avó\n",
      "current_line:28809, duration = 0.17297792434692383\n",
      "Analogy --> ['pai', 'mãe', 'príncipe', 'princesa']\n",
      "prediction --> ornamentados\n",
      "current_line:28810, duration = 0.18981313705444336\n",
      "Analogy --> ['pai', 'mãe', 'filho', 'filha']\n",
      "prediction --> morte\n",
      "current_line:28811, duration = 0.20179438591003418\n",
      "Analogy --> ['pai', 'mãe', 'padrasto', 'madrasta']\n",
      "prediction --> Orquestra\n",
      "current_line:28814, duration = 0.18649029731750488\n",
      "Analogy --> ['pai', 'mãe', 'tio', 'tia']\n",
      "prediction --> mordida\n",
      "current_line:28816, duration = 0.18265104293823242\n",
      "Analogy --> ['pai', 'mãe', 'rapaz', 'moça']\n",
      "prediction --> harpa\n",
      "current_line:28817, duration = 0.18529891967773438"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "initial_time = time.time()\n",
    "with open(file_path) as inputfile:\n",
    "    for line in inputfile:\n",
    "        total_lines += 1\n",
    "        initial = time.time()\n",
    "        list_line = line.strip().split()\n",
    "        if all([word in vocab for word in list_line]):\n",
    "            valid_tests += 1\n",
    "            analogue = analogy(list_line[0],\n",
    "                               list_line[1],\n",
    "                               list_line[2],\n",
    "                               reverse_dictionary,\n",
    "                               dictionary,\n",
    "                               final_embeddings)\n",
    "            print(\"\\nAnalogy -->\", list_line)\n",
    "            print(\"prediction -->\", analogue)\n",
    "            if analogue == list_line[3]:\n",
    "                correct_answer += 1\n",
    "                print(\"\\nYESSSSSSSSSSSSSSSSSSSSS\\n\")\n",
    "            current_time = time.time() - initial\n",
    "            sys.stdout.write('\\rcurrent_line:{}, duration = {}'.format(total_lines,\n",
    "                                                                        current_time))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "print(\"\\ntotal_lines = {}\".format(total_lines))\n",
    "print(\"valid_tests = {}\".format(valid_tests))\n",
    "print(\"correct_answer = {}\".format(correct_answer))\n",
    "print(\"total_loss = {}\".format(total_loss))\n",
    "print(\"duration = {}\".format(time.time() - initial_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mulher'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[414]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
