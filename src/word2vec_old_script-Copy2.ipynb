{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another implementation of word2vec: this is the first version that I saw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import inspect\n",
    "import string\n",
    "import time\n",
    "from word2vec import Config, DataHolder\n",
    "from data_process import batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'pt96.txt'  \n",
    "file_path = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "file_path = os.path.join(file_path,'data')\n",
    "file_path = os.path.join(file_path,filename)\n",
    "\n",
    "my_data = DataHolder(file_path)\n",
    "vocab_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, count, word2index, index_to_word = my_data.build_data(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conto Contos UNK 1870 Contos UNK Textofonte Obra Completa Machado de Assis vol II Rio de Janeiro Nova Aguilar 1994 Publicado originalmente pela Editora Garnier Rio de Janeiro em 1870 ÍNDICE UNK UNK UNK UNK A UNK DE UNK O UNK DE UNK UNK DE UMA UNK UNK UNK UNK E UNK UNK FREI UNK UNK UNK ÍNDICE Capítulo Primeiro Capítulo II Capítulo iii Capítulo UNK Capítulo v Capítulo UNK Capítulo UNK CAPÍTULO VIII CAPÍTULO PRIMEIRO Era conveniente ao romance que o leitor ficasse muito tempo sem saber quem era Miss Dollar Mas por outro lado sem a apresentação de "
     ]
    }
   ],
   "source": [
    "for i in data[:100]:\n",
    "    print(index_to_word[i], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocab_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "  \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases,train_labels, embed,num_sampled, vocab_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 7.48771047592\n",
      "Nearest to parte: Sur·, sol, 20131, onde, SP88, colocar, genuínas, Consegue,\n",
      "Nearest to para: atrasou, Miz, corrija, Mariz, poderio, Coco, Yzerman, posteridade,\n",
      "Nearest to dia: doutrinas, verificamos, demarcada, originouse, perfazem, encontram, Friday, Demo,\n",
      "Nearest to dois: Carlos, 58°, indenizações, estabeleceram, Stravinsky, lembrase, excede, grava,\n",
      "Nearest to das: Johnson·, extensão, vicariato, apurada, Patton, cuidando, Atlanta, polegar,\n",
      "Nearest to e: gruta, súplicas, reconstrução, confiadas, interna, reportou, começaram, transpiração,\n",
      "Nearest to tempo: Rhythm, hospedeiros, Image, SocialDemocrata, Diva, Dolly, matador, ALTENER,\n",
      "Nearest to É: suspense, Resolveu, mistério, subjetividade, forçaram, Tele, satisfeita, Wendy,\n",
      "Nearest to está: Sul3, cedido, tão, Mandato, Praticamente, Andrei, trato, alla,\n",
      "Nearest to até: chorava, Heian, corrigido, veria, Itsuki, colônias, desfile, monástico,\n",
      "Nearest to na: Pride, encarceramento, patrick, concórdia, cegos, Esperar, coros, Pertence,\n",
      "Nearest to ser: Finlandesa, Aquelas, 1615, Heidelberger, Benito, lucros, surgido, Martins,\n",
      "Nearest to grande: convertida, inspirar, rijo, Rumpelstiltskin, desafiante, suicidas, supressão, Comuns,\n",
      "Nearest to tem: 01km², convergem, cavalheiros, Shin, teclado, Gênio, 202, nasceram,\n",
      "Nearest to ter: cometeram, camuflagem, faziao, crucifixo, impor, retábulos, NV, discreto,\n",
      "Nearest to a: acessar, basilar, Ferreira, relançou, Olímpica, botica, arquipélagos, Vingadores,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "data_index = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  ts = time.time()\n",
    "  print(\"Initialized\")\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    data_index,batch_data, batch_labels = batch_generator(\n",
    "      batch_size, num_skips, skip_window,data_index,data)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step\", step, \":\", average_loss)\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = index_to_word[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = \"Nearest to %s:\" % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = index_to_word[nearest[k]]\n",
    "          log = \"%s %s,\" % (log, close_word)\n",
    "        print(log)\n",
    "  te = time.time()\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"duration= \", te-ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def analogy(w1,w2,w3,index_to_word,word2index, embeddings):\n",
    "    a = embeddings[word2index[w1]]\n",
    "    b = embeddings[word2index[w2]]\n",
    "    c = embeddings[word2index[w3]]\n",
    "    def apply_dot(x):\n",
    "        return b.dot(x) - a.dot(x) + c.dot(x)\n",
    "    all_results = [(apply_dot(w), index) for index, w in enumerate(embeddings) if (index!= word2index[w1] and index!= word2index[w2] and index!= word2index[w3])]\n",
    "    all_results.sort(reverse=True)\n",
    "    result = [(index_to_word[index],value) for (value,index) in all_results[0:11]]\n",
    "    return result[0][0]  \n",
    "\n",
    "def top_k_sim(vector, index_to_word, embeddings,k=10):\n",
    "    all_sim = [(1 - spatial.distance.cosine(vector, w),index) for index, w in enumerate(embeddings)]\n",
    "    all_sim.sort(reverse =True)\n",
    "    result = [(index_to_word[index],value) for (value,index) in all_sim[0:k+1]]\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mulher = final_embeddings[word2index['mulher']]\n",
    "top_k_sim(mulher,index_to_word,final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_file=\"AnalogiesBr_little.txt\"\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "prefix = os.path.join(currentdir, \"evaluation\")\n",
    "file_path = os.path.join(prefix, eval_file)\n",
    "valid_tests = 0\n",
    "correct_answer = 0\n",
    "total_lines = 0\n",
    "total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "initial_time = time.time()\n",
    "with open(file_path) as inputfile:\n",
    "    for line in inputfile:\n",
    "        total_lines += 1\n",
    "        initial = time.time()\n",
    "        list_line = line.strip().split()\n",
    "        if all([word in word2index for word in list_line]):\n",
    "            valid_tests += 1\n",
    "            analogue = analogy(list_line[0],\n",
    "                               list_line[1],\n",
    "                               list_line[2],\n",
    "                               index_to_word,\n",
    "                               word2index,\n",
    "                               final_embeddings)\n",
    "            print(\"\\nAnalogy -->\", list_line)\n",
    "            print(\"prediction -->\", analogue)\n",
    "            if analogue == list_line[3]:\n",
    "                correct_answer += 1\n",
    "                print(\"\\nYESSSSSSSSSSSSSSSSSSSSS\\n\")\n",
    "            current_time = time.time() - initial\n",
    "            sys.stdout.write('\\rcurrent_line:{}, duration = {}'.format(total_lines,\n",
    "                                                                        current_time))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "print(\"\\ntotal_lines = {}\".format(total_lines))\n",
    "print(\"valid_tests = {}\".format(valid_tests))\n",
    "print(\"correct_answer = {}\".format(correct_answer))\n",
    "print(\"total_loss = {}\".format(total_loss))\n",
    "print(\"duration = {}\".format(time.time() - initial_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "26/342"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
